services:
  transcriber:
    build:
      context: .
      dockerfile: ./vllm/Dockerfile.transcriber
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - HUGGING_FACE_HUB_TOKEN
      - VLLM_MAX_AUDIO_CLIP_FILESIZE_MB=10000
    ipc: "host"
    volumes:
      - ${VLLM_CACHE:-/root/.cache/huggingface}:/root/.cache/huggingface
    command: >
      --model ${TRANSCRIBER_MODEL:-openai/whisper-large-v3-turbo}
      --trust-remote-code
      --async-scheduling
      --gpu_memory_utilization 0.3
    # ports:
    #   - ${TRANSCRIBER_PORT:-8002}:8000
